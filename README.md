# Neural Networks

![image](https://github.com/jd99davidson/Neural_Networks/assets/26396740/e807dd71-0416-4587-9194-4d081454abbf)


This repository contains the perceptron.py file which contains two classes: Perceptron() and MultiLayeredPerceptron(). The MultiLayeredPerceptron() class uses composition to nest the layers of individual neurons (perceptrons) in it's network attribute. There are two primary methods for the perceptron class: run() and bp(). Because the weights of the neurons are stored in the instantiated perceptrons in the network attribtue, they can be accessed and updated using the backpropogation algorithm to train the weights to produce the desired outcome of a sample. Therefore, the bp() method is used to train the network ran for as many training epochs is desired. The run() method then takes in the appropriate number of inputs to feed through the network and returns the outputs the neural network thinks is appropriate. 

This repository also provides three examples of use cases for the neural network. The first is a series of logic gates that can be solved simply by providing the appropriate weights to a single perceptron. The network then behaves as the logical 'OR', 'AND', and 'XOR' gates. The second is the Optical Character Recognition neural network for a seven-segment display. This trains the MultilayerPerceptron instance on sample data, mapping the input of the seven-segment display pattern to a single integer. Even though the network has only seen the ideal circumstances, it is still able to accurately categorize the inputs if they deviate from the ideal samplpe set. The third network is a simple training pattern of the XOR gate, as opposed to providing the weights as in the logical_gates.py file.
